services:
  # PostgreSQL with pgvector for GraphRAG data
  postgres:
    image: pgvector/pgvector:pg16
    container_name: docqa-postgres
    ports:
      - "5433:5432"  # Use 5433 to avoid conflict with host PostgreSQL
    environment:
      - POSTGRES_DB=docqa
      - POSTGRES_USER=docqa
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-docqa_secret}
    volumes:
      - ~/.docqa/postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U docqa -d docqa"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Backend API - FastAPI with PostgreSQL + pgvector
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    image: docqa-backend:local
    container_name: docqa-backend
    ports:
      - "8080:8000"  # Use 8080 to avoid conflict with Kotaemon on 8000
    environment:
      - DATABASE_URL=postgresql+asyncpg://docqa:${POSTGRES_PASSWORD:-docqa_secret}@docqa-postgres:5432/docqa
      - VLLM_CHAT_URL=http://vllm-chat:8000/v1
      - VLLM_EMBED_URL=http://vllm-embed:8000/v1
      - LLM_MODEL=Qwen/Qwen2.5-7B-Instruct
      - EMBEDDING_MODEL=BAAI/bge-m3
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped

  # Ollama - DEPRECATED, kept only for Kotaemon Gradio UI compatibility
  # Backend now uses vLLM directly
  ollama:
    image: ollama/ollama:latest
    pull_policy: always
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ~/.docqa/ollama:/root/.ollama
    environment:
      - OLLAMA_CONTEXT_LENGTH=32768
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_KV_CACHE_TYPE=q8_0
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_DEBUG=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Kotaemon - Multimodal RAG with Docling (CUDA-enabled)
  # NOTE: Backend API is now separate (backend service above)
  # Kotaemon is kept for its Gradio UI and Docling integration
  kotaemon:
    build:
      context: .
      dockerfile: Dockerfile.kotaemon-cuda
    image: kotaemon-cuda:local
    container_name: kotaemon
    ports:
      - "3000:7860"
    dns:
      - 8.8.8.8
      - 8.8.4.4
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ~/.docqa/kotaemon:/app/ktem_app_data
      # Patch for local VLM support
      - ./gpt4v_patched.py:/app/libs/kotaemon/kotaemon/loaders/utils/gpt4v.py:ro
      # Patch for GraphRAG local endpoint support
      - ./graphrag_pipelines_patched.py:/app/libs/ktem/ktem/index/file/graph/pipelines.py:ro
      # Patch for Docling GPU acceleration
      - ./docling_loader_patched.py:/app/libs/kotaemon/kotaemon/loaders/docling_loader.py:ro
      # GraphRAG settings for local Ollama
      - ./graphrag_settings.yaml:/app/settings.yaml.example:ro
      # Test scripts
      - ./tests:/app/tests:ro
      # Benchmark scripts
      - ./benchmarks:/app/benchmarks:ro
      # Benchmark output (writable)
      - ./benchmark_runs:/app/benchmark_runs
    environment:
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      # Local Ollama models
      - LOCAL_MODEL=qwen2.5:14b
      - LOCAL_MODEL_EMBEDDINGS=bge-m3
      - KH_OLLAMA_URL=http://ollama:11434/v1/
      # Local VLM for figure captioning during PDF ingestion
      - KH_VLM_ENDPOINT=http://ollama:11434/v1/chat/completions
      - KH_VLM_MODEL=qwen2.5vl:7b
      # Enable multimodal document processing
      - KH_REASONINGS_USE_MULTIMODAL=true
      # MS GraphRAG config - use Ollama as OpenAI-compatible endpoint
      - GRAPHRAG_API_KEY=ollama
      - GRAPHRAG_API_BASE=http://ollama:11434/v1
      - GRAPHRAG_EMBEDDING_MODEL=bge-m3
      - USE_CUSTOMIZED_GRAPHRAG_SETTING=true
    depends_on:
      ollama:
        condition: service_healthy
    # Default entrypoint runs Gradio UI
    restart: unless-stopped

  # vLLM LLM - for models not supported by Ollama (e.g. LFM2)
  # Start with: docker compose --profile vllm up -d vllm-llm vllm-embed vllm-chat lfm2-adapter
  vllm-llm:
    image: vllm/vllm-openai:latest
    pull_policy: always
    container_name: vllm-llm
    profiles:
      - vllm
    ports:
      - "8001:8000"
    volumes:
      - vllm_cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_SERVER_DEV_MODE=1  # Required for sleep mode
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # LFM2-Extract for structured entity extraction
    # LFM2-1.2B-Extract has 128K native context - using 8K with 15% GPU (~3.7GB)
    # GraphRAG chunks are ~500 tokens + prompt ~1000, so 8K is sufficient
    command: ["--model", "LiquidAI/LFM2-1.2B-Extract", "--max-model-len", "8192", "--gpu-memory-utilization", "0.15", "--max-num-seqs", "4"]
    restart: unless-stopped

  # vLLM Embeddings - BGE-M3 for vector embeddings
  vllm-embed:
    image: vllm/vllm-openai:latest
    pull_policy: always
    container_name: vllm-embed
    profiles:
      - vllm
    ports:
      - "8003:8000"
    volumes:
      - vllm_cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_SERVER_DEV_MODE=1  # Required for sleep mode
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # BGE-M3 embeddings - minimal allocation to coexist with other models
    command: ["--model", "BAAI/bge-m3", "--gpu-memory-utilization", "0.06", "--max-num-seqs", "8"]
    restart: unless-stopped

  # vLLM Summarizer - Qwen2.5-7B for community reports and Q&A
  vllm-chat:
    image: vllm/vllm-openai:latest
    pull_policy: always
    container_name: vllm-chat
    profiles:
      - vllm
    ports:
      - "8004:8000"
    volumes:
      - vllm_cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_SERVER_DEV_MODE=1  # Required for sleep mode
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # Qwen2.5-7B for community reports and Q&A
    # Model needs ~14GB for weights, 70% allocation (~16.8GB) to coexist with other vLLM services
    command: ["--model", "Qwen/Qwen2.5-7B-Instruct", "--max-model-len", "8192", "--gpu-memory-utilization", "0.70", "--max-num-seqs", "8"]
    restart: unless-stopped

  # LFM2 Adapter - converts between GraphRAG format and LFM2-Extract JSON
  lfm2-adapter:
    image: python:3.11-slim
    container_name: lfm2-adapter
    profiles:
      - vllm
    ports:
      - "8002:8002"
    volumes:
      - ./benchmarks/lfm2_adapter.py:/app/lfm2_adapter.py:ro
    working_dir: /app
    command: ["sh", "-c", "pip install -q fastapi uvicorn httpx && python lfm2_adapter.py"]
    depends_on:
      - vllm-llm
    restart: unless-stopped

  # Indexer - GPU-aware pipeline orchestrator
  # Runs document indexing stages: OCR → Entity Extraction → Community Reports → Embeddings
  # Uses vLLM sleep mode to swap models between stages
  indexer:
    build:
      context: .
      dockerfile: indexer/Dockerfile
    image: docqa-indexer:local
    container_name: indexer
    profiles:
      - vllm
    volumes:
      # Job state and outputs
      - ~/.docqa/indexer:/app/indexer_jobs
      # Input data directory
      - ./tests/data:/data:ro
      # Indexer source (for development)
      - ./indexer:/app/indexer:ro
      # GraphRAG settings
      - ./graphrag_settings.yaml:/app/graphrag_settings.yaml:ro
      # Share kotaemon data (includes Docling models)
      - ~/.docqa/kotaemon:/app/ktem_app_data
      # Docker socket for container control
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - PYTHONUNBUFFERED=1
      - GRAPHRAG_API_KEY=ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - vllm-llm
      - vllm-embed
      - vllm-chat
      - lfm2-adapter
    # Keep container running for interactive use
    entrypoint: ["tail", "-f", "/dev/null"]
    restart: unless-stopped

volumes:
  # Only vllm_cache remains as a Docker volume (model weights, can be re-downloaded)
  # All other data uses bind mounts to ~/.docqa/ for persistence
  vllm_cache:
