# Chat Frontend Roadmap: Single User to Secure Multi-User Scale

## Current State Summary

| Component | Status | Gap |
|-----------|--------|-----|
| Frontend chat UI | **DONE** | React + Zustand, working |
| SSE streaming | **DONE** | Chat streams work end-to-end |
| Collection selector | **DONE** | Demo collection (id=4) works |
| Backend chat API | **DONE** | GraphRAG retrieval + Ollama streaming |
| Conversations UI | **DONE** | List, create, rename, delete in sidebar |
| Document upload UI | Stubbed | Frontend simulates progress, no backend |
| Conversation persistence | **NOT DONE** | Frontend-only (localStorage), no DB |
| Indexing pipeline | **DONE** | Works via CLI |
| Indexing API trigger | **NOT DONE** | No HTTP endpoint to start jobs |
| Progress streaming | **NOT DONE** | No SSE from indexer to frontend |
| Auth | Stubbed | Anonymous user hardcoded |
| User isolation | **NOT DONE** | No user_id on collections |

### What's Already Working
- Chat with demo collection: **fully functional**
- Collection switching: works
- Message streaming: works
- Sources display: works
- Conversation list UI: works (but localStorage only)

## Architecture Constraint: Single GPU

```
Indexing (10+ minutes)          vs      Inference (<5 seconds)
├── OCR: Docling (8GB)                  ├── Embed query: BGE-M3 (2.5GB)
├── Entity: LFM2 (3GB)                  └── LLM: Qwen via Ollama (14GB)
├── Community: Qwen (14GB)
└── Embed: BGE-M3 (2.5GB)
```

**Reality**: A user dragging files and asking a question triggers BOTH sequentially on the same GPU. We must manage this gracefully.

---

## [x] Phase 1: Chat Working

The chat system is **already fully functional**:
- Frontend: React + Zustand + SSE streaming
- Backend: GraphRAG retrieval + Ollama LLM
- Demo collection (id=4) indexed and queryable

```bash
# Verify (already works)
curl http://localhost:8080/api/collections
curl -N http://localhost:8080/api/chat -d '{"message": "test", "collection_id": 4}'
```

---

## [ ] Phase 2: Conversation Persistence + Context Management

**Goal**: Chat history persists, context carries across messages.

### [ ] 2.1 Database Schema
```sql
-- Add to backend/schema.sql
CREATE TABLE conversations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    collection_id INTEGER REFERENCES collections(id) ON DELETE CASCADE,
    user_id VARCHAR(64) DEFAULT 'anonymous',  -- Future: auth user
    title VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    conversation_id UUID REFERENCES conversations(id) ON DELETE CASCADE,
    role VARCHAR(20) NOT NULL,  -- 'user' | 'assistant'
    content TEXT NOT NULL,
    sources JSONB,  -- Stored retrieval sources
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_conversations_user ON conversations(user_id, updated_at DESC);
CREATE INDEX idx_messages_conversation ON messages(conversation_id, created_at);
```

### [ ] 2.2 Backend Endpoints
```python
# New endpoints in backend/main.py

@app.post("/api/conversations")
async def create_conversation(collection_id: int, title: str = None):
    # Insert row, return {id, collection_id, title, created_at}

@app.get("/api/conversations")
async def list_conversations(collection_id: int = None):
    # Return conversations, optionally filtered by collection

@app.get("/api/conversations/{id}")
async def get_conversation(id: str):
    # Return conversation with messages

@app.delete("/api/conversations/{id}")
async def delete_conversation(id: str):
    # Cascade deletes messages

@app.patch("/api/conversations/{id}")
async def update_conversation(id: str, title: str):
    # Rename conversation
```

### [ ] 2.3 Context Management Strategy

**Problem**: LLM needs conversation history, but can't send unlimited tokens.

**Solution**: Rolling context window with retrieval augmentation.

```python
class ChatRequest(BaseModel):
    message: str
    collection_id: int
    conversation_id: Optional[str] = None  # New field

@app.post("/api/chat")
async def chat(request: ChatRequest):
    # 1. Load last N messages from conversation (e.g., 10)
    history = await load_conversation_history(request.conversation_id, limit=10)

    # 2. Retrieve context for CURRENT query only
    context = await retriever.retrieve(request.message, request.collection_id)

    # 3. Build prompt with history + fresh context
    prompt = build_prompt(
        system=SYSTEM_PROMPT,
        context=context.to_prompt_context(),
        history=history,  # [{role, content}, ...]
        query=request.message
    )

    # 4. Stream response, save to messages table
    async for chunk in stream_llm(prompt):
        yield chunk

    # 5. Save user message + assistant response
    await save_messages(request.conversation_id, request.message, full_response)
```

**Token Budget**:
- History: 2000 tokens (last ~10 messages)
- Retrieved context: 4000 tokens
- System prompt: 500 tokens
- Query + response: 1500 tokens
- **Total**: ~8000 tokens (fits Qwen 32K context)

### [ ] 2.4 Frontend Changes
- `useConversations.ts` - Already has create/list/delete logic
- `chatStore.ts` - Add `conversation_id` to state
- `useChat.ts` - Include `conversation_id` in requests

**Files**: `backend/schema.sql`, `backend/main.py`, `frontend/src/hooks/useChat.ts`

---

## [ ] Phase 3: Document Upload + Indexing with Progress

**Goal**: User uploads docs in chat → creates collection → shows progress → can query.

### [ ] 3.1 Backend: Collection Creation Endpoint
```python
@app.post("/api/collections")
async def create_collection(
    name: str = Form(...),
    files: list[UploadFile] = File(...)
):
    """Create collection and trigger indexing. Returns SSE progress stream."""
    # 1. Generate job_id, save files to indexer input directory
    job_id = str(uuid.uuid4())[:8]
    job_dir = Path(f"/app/indexer_jobs/{job_id}")
    input_dir = job_dir / "input"
    input_dir.mkdir(parents=True)

    for file in files:
        dest = input_dir / file.filename
        async with aiofiles.open(dest, 'wb') as f:
            await f.write(await file.read())

    # 2. Create job.json
    job = IndexJob(
        job_id=job_id,
        name=name,
        input_files=[str(p) for p in input_dir.glob("*")],
        status="pending"
    )
    job.save()

    # 3. Return SSE stream that monitors job progress
    return StreamingResponse(
        stream_job_progress(job_id),
        media_type="text/event-stream"
    )

async def stream_job_progress(job_id: str):
    """Poll job.json and emit progress events."""
    while True:
        job = IndexJob.load(job_id)

        progress = IndexingProgress(
            phase=job.current_stage or "pending",
            chunks_total=job.stages.get("ocr", {}).get("stats", {}).get("chunks_total", 0),
            chunks_done=job.stages.get("ocr", {}).get("stats", {}).get("chunks_done", 0),
            entities_found=job.stages.get("entity_extraction", {}).get("stats", {}).get("entities", 0),
            percent_complete=calculate_percent(job),
            eta_seconds=estimate_eta(job),
            files=[...],
        )

        yield f"event: {progress.phase}\ndata: {progress.json()}\n\n"

        if job.status in ("completed", "failed"):
            if job.status == "completed":
                # Import to PostgreSQL
                collection_id = await import_to_postgres(job_id, name)
                yield f"event: complete\ndata: {{'collection_id': {collection_id}}}\n\n"
            else:
                yield f"event: error\ndata: {{'error': '{job.error}'}}\n\n"
            break

        await asyncio.sleep(2)  # Poll interval
```

### [ ] 3.2 Indexer: API Trigger (vs CLI)
```python
# indexer/api.py - New file
from fastapi import FastAPI
from .orchestrator import PipelineOrchestrator

app = FastAPI()

@app.post("/jobs/{job_id}/start")
async def start_job(job_id: str, from_stage: str = None):
    """Start indexing job. Called by backend after upload."""
    orchestrator = PipelineOrchestrator(job_id)
    # Run in background task
    asyncio.create_task(orchestrator.run(from_stage=from_stage))
    return {"status": "started"}

@app.get("/jobs/{job_id}/status")
async def job_status(job_id: str):
    """Return current job state."""
    job = IndexJob.load(job_id)
    return job.to_dict()
```

### [ ] 3.3 GPU Coordination: Queue Inference During Indexing

**Problem**: User uploads doc, indexing starts (uses GPU), then asks a question (needs GPU).

**Solutions** (in order of complexity):

**Option A: Block inference during indexing (simplest)**
```python
@app.post("/api/chat")
async def chat(request: ChatRequest):
    if await is_indexing_active():
        raise HTTPException(503, "Indexing in progress. Please wait.")
```

**Option B: Queue inference requests**
```python
# Use asyncio.Lock or Redis-based lock
gpu_lock = asyncio.Lock()

@app.post("/api/chat")
async def chat(request: ChatRequest):
    async with gpu_lock:  # Wait for indexing to release
        # ... normal retrieval + inference
```

**Option C: Pause indexing for inference (advanced)**
```python
# Indexer checks for pending inference requests between stages
async def run_stage(stage):
    await wait_for_inference_queue_empty()
    # ... run stage
```

**Recommended**: Start with Option A, evolve to B.

### [ ] 3.4 Frontend: Progress UI
```typescript
// Already defined in types/index.ts:
interface IndexingProgress {
  phase: 'chunking' | 'extracting' | 'embedding' | 'complete' | 'error';
  chunks_total: number;
  chunks_done: number;
  entities_found: number;
  eta_seconds: number;
  percent_complete: number;
  files: { name: string; status: 'pending' | 'processing' | 'done'; chunks?: number }[];
  error_message?: string;
}
```

**New Component**: `IndexingProgressModal.tsx`
- Progress bar with percentage
- ETA countdown
- Phase indicator (OCR → Entities → Communities → Embeddings)
- Per-file status
- "Notify me" checkbox → Browser notification on complete
- Beep sound on complete

```typescript
// Browser notification
if (progress.phase === 'complete' && notifyEnabled) {
  new Notification('Indexing Complete', {
    body: `${collectionName} is ready to query`,
    icon: '/icon.png'
  });
  new Audio('/beep.mp3').play();
}
```

**Files**: `backend/main.py`, `indexer/api.py` (new), `frontend/src/components/IndexingProgressModal.tsx` (new)

---

## [ ] Phase 4: Multi-User Authentication

**Goal**: Users authenticate, see only their collections.

### [ ] 4.1 Database Schema
```sql
CREATE TABLE users (
    id VARCHAR(64) PRIMARY KEY,  -- UUID or OAuth sub
    email VARCHAR(255) UNIQUE,
    username VARCHAR(100),
    password_hash VARCHAR(255),  -- For local auth
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Add user_id to collections
ALTER TABLE collections ADD COLUMN user_id VARCHAR(64) REFERENCES users(id);
ALTER TABLE conversations ADD COLUMN user_id VARCHAR(64) REFERENCES users(id);
```

### [ ] 4.2 Authentication Strategy

**Option A: Simple JWT (recommended for start)**
```python
# backend/auth.py
from fastapi import Depends, HTTPException
from fastapi.security import HTTPBearer
import jwt

security = HTTPBearer()

async def get_current_user(token = Depends(security)):
    try:
        payload = jwt.decode(token.credentials, SECRET_KEY, algorithms=["HS256"])
        return payload["user_id"]
    except:
        raise HTTPException(401, "Invalid token")

@app.post("/api/collections")
async def create_collection(..., user_id: str = Depends(get_current_user)):
    # user_id automatically injected
```

**Option B: OAuth (Google/GitHub) - later**
```python
# Use authlib or python-social-auth
```

### [ ] 4.3 Data Isolation
```python
# All queries filtered by user_id
@app.get("/api/collections")
async def list_collections(user_id: str = Depends(get_current_user)):
    result = await db.execute(
        "SELECT * FROM collections WHERE user_id = :user_id",
        {"user_id": user_id}
    )
    return [...]
```

**Files**: `backend/auth.py` (new), `backend/schema.sql`, `backend/main.py`

---

## [ ] Phase 5: Scaling Blueprint

### [ ] 5.1 Bottleneck Analysis

| Bottleneck | Impact | Solution |
|------------|--------|----------|
| Single GPU | Indexing blocks inference | Multi-GPU or cloud burst |
| Single indexer | One job at a time | Worker pool (Ray/Celery) |
| No query cache | Repeated work | Redis cache layer |
| Synchronous OCR | Blocks pipeline | Streaming + parallelism |
| PostgreSQL connections | 10 default | Increase pool, read replicas |

### [ ] 5.2 Scaling Architecture (10+ Users)

```
                    ┌─────────────────┐
                    │   Load Balancer │
                    └────────┬────────┘
                             │
         ┌───────────────────┼───────────────────┐
         │                   │                   │
    ┌────▼────┐        ┌────▼────┐        ┌────▼────┐
    │ Backend │        │ Backend │        │ Backend │
    │   #1    │        │   #2    │        │   #3    │
    └────┬────┘        └────┬────┘        └────┬────┘
         │                   │                   │
         └───────────────────┼───────────────────┘
                             │
              ┌──────────────┼──────────────┐
              │              │              │
         ┌────▼────┐   ┌────▼────┐   ┌────▼────┐
         │ Redis   │   │ Postgres│   │ Postgres│
         │ Cache   │   │ Primary │   │ Replica │
         └─────────┘   └─────────┘   └─────────┘
                             │
         ┌───────────────────┼───────────────────┐
         │                   │                   │
    ┌────▼────┐        ┌────▼────┐        ┌────▼────┐
    │ Indexer │        │ Indexer │        │ Indexer │
    │ Worker  │        │ Worker  │        │ Worker  │
    │ (GPU 1) │        │ (GPU 2) │        │ (GPU 3) │
    └─────────┘        └─────────┘        └─────────┘
```

### [ ] 5.3 Immediate Optimizations (Before Multi-GPU)

1. **Query Cache** (Redis)
   ```python
   cache_key = f"query:{hash(query)}:{collection_id}"
   cached = await redis.get(cache_key)
   if cached:
       return cached
   # ... retrieve + cache for 5 minutes
   ```

2. **Batch Embeddings**
   ```python
   # Instead of embedding one text at a time
   embeddings = await embed_batch([tu["text"] for tu in candidates])
   ```

3. **Pre-warm Collections**
   - Cache top community reports at startup
   - Pre-compute entity name index

4. **Connection Pooling**
   ```python
   # Increase from 10 to 50
   engine = create_async_engine(DATABASE_URL, pool_size=50, max_overflow=100)
   ```

### [ ] 5.4 Security Checklist

| Layer | Measure |
|-------|---------|
| Transport | HTTPS everywhere |
| Auth | JWT with short expiry (15m) + refresh tokens |
| Data | Row-level security via user_id filter |
| Files | Isolated storage per user: `/data/{user_id}/` |
| API | Rate limiting per user |
| Secrets | Env vars, not in code |
| Indexer | Jobs tagged with user_id, validated on access |

---

## Implementation Roadmap

| Phase | Deliverable | Status |
|-------|-------------|--------|
| [x] **1** | Chat works with demo collection | DONE |
| [ ] **2** | Conversation persistence + context | Next |
| [ ] **3** | Document upload + progress UI | Planned |
| [ ] **4** | Multi-user auth | Planned |
| [ ] **5** | Scaling optimizations | Ongoing |

### Phase 2 Files
- `backend/schema.sql` - Add conversations, messages tables
- `backend/main.py` - Add CRUD endpoints, modify chat for history
- `frontend/src/hooks/useChat.ts` - Include conversation_id

### Phase 3 Files
- `backend/main.py` - Add POST `/api/collections` with SSE progress
- `indexer/api.py` - New API for job management
- `frontend/src/components/IndexingProgressModal.tsx` - New progress UI

### Phase 4 Files
- `backend/auth.py` - New auth module
- `backend/schema.sql` - Add users table, user_id columns
- `frontend/src/api/client.ts` - Token management

---

## Verification Plan

### Phase 1
```bash
# Backend serves collections
curl http://localhost:8080/api/collections

# Chat streams correctly
curl -N http://localhost:8080/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "test", "collection_id": 4}'
```

### Phase 2
```bash
# Create conversation
curl -X POST http://localhost:8080/api/conversations \
  -d '{"collection_id": 4}'

# Chat with conversation context
curl -N http://localhost:8080/api/chat \
  -d '{"message": "test", "collection_id": 4, "conversation_id": "abc123"}'

# Verify history persisted
curl http://localhost:8080/api/conversations/abc123
```

### Phase 3
```bash
# Upload document, watch progress
curl -X POST http://localhost:8080/api/collections \
  -F "name=Test Collection" \
  -F "files=@document.pdf"
# Returns SSE stream with progress events
```

### Phase 4
```bash
# Login
curl -X POST http://localhost:8080/api/auth/login \
  -d '{"email": "user@example.com", "password": "secret"}'
# Returns JWT

# Access with token
curl http://localhost:8080/api/collections \
  -H "Authorization: Bearer <token>"
# Returns only user's collections
```
